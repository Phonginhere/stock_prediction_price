{"cells":[{"source":"import os\nimport pandas as pd\nimport yfinance as yf\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler","metadata":{"executionCancelledAt":null,"executionTime":50,"lastExecutedAt":1731474852400,"lastExecutedByKernel":"b758ff07-d9e6-42ac-a379-9fac2235c78a","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import os\nimport pandas as pd\nimport yfinance as yf\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler"},"cell_type":"code","id":"363d4aee-6cf3-4c1b-bff7-fe87dc135b09","outputs":[],"execution_count":41},{"source":"def load_process_data(ticker, start_date, end_date, dealNaN=True, drop_NaN=True, split_by_ratio=True, split_by_date=False, split_by_randomly=False, split_date=None, scale=True):\n    DATA_DIR = 'stock_data'\n    \n    test_size = 0.2\n    feature_columns = [\"Open\",\"High\",\"Low\",\"Close\",\"Adj Close\"]\n    \n    # Check if the data directory exists, if not, create it\n    if not os.path.exists(DATA_DIR):\n        os.makedirs(DATA_DIR)\n    \n    # Create a file path for the stock data\n    file_path = os.path.join(DATA_DIR, f'{ticker}_{start_date}_{end_date}.csv')\n    \n    # Load or download data\n    if os.path.exists(file_path):\n        print(f\"Loading data from {file_path}\")\n        data = pd.read_csv(file_path, index_col='Date', parse_dates=True)\n    else:\n        print(f\"Downloading data for {ticker} from Yahoo finance\")\n        data = yf.download(ticker, start=start_date, end=end_date)\n        data.to_csv(file_path)\n        print(f\"Data saved to {file_path}\")\n    \n    result = {'data': data.copy()}\n    \n    # Handle NaN values\n    if dealNaN:\n        # Check for NaN values before handling\n        print(\"NaN values before handling\")\n        print(data.isna().sum())\n        # Depends on chosen drop or fill NaN\n        if drop_NaN is True:\n            data.dropna(inplace=True)\n        else:\n            data.fillna(data.mean(), inplace=True)\n        # Check for NaN values after handling\n        print(\"NaN values after handling\")\n        print(data.isna().sum())\n    \n    # Split data with specified method\n    if split_by_ratio:\n        print('Splitting by ratio')\n        train_samples = int((1-test_size) * len(data))\n        \n        result['data_train'] = data[:train_samples]\n        result['data_test'] = data[train_samples:]\n        \n        print(f\"Training samples: {len(result['data_train'])}\")\n        print(f\"Test samples: {len(result['data_test'])}\")\n        \n    elif split_by_date and split_date is not None:\n        print('Splitting by date')\n        split_date = pd.to_datetime(split_date)\n        \n        result['data_train'] = data[data.index <= split_date]\n        result['data_test'] = data[data.index > split_date]\n        \n        print(f\"Training samples: {len(result['data_train'])}\")\n        print(f\"Test samples: {len(result['data_test'])}\")\n    elif split_by_randomly:\n        print('Randomly splitting with shuffling')\n        result['data_train'], result['data_test'] = train_test_split(data, test_size=test_size, shuffle=True)\n        \n        print(f\"Training samples: {len(result['data_train'])}\")\n        print(f\"Test samples: {len(result['data_test'])}\")\n    \n    # Optionally scale feature columns\n    if scale:\n        scaler = MinMaxScaler(feature_range=(0,1))\n        data[feature_columns] = scaler.fit_transform(data[feature_columns])\n        result['scaled_data'] = data[feature_columns]\n        \n    return result","metadata":{"executionCancelledAt":null,"executionTime":56,"lastExecutedAt":1731474852456,"lastExecutedByKernel":"b758ff07-d9e6-42ac-a379-9fac2235c78a","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"def load_process_data(ticker, start_date, end_date, dealNaN=True, drop_NaN=True, split_by_ratio=True, split_by_date=False, split_by_randomly=False, split_date=None, scale=True):\n    DATA_DIR = 'stock_data'\n    \n    test_size = 0.2\n    feature_columns = [\"Open\",\"High\",\"Low\",\"Close\",\"Adj Close\"]\n    \n    # Check if the data directory exists, if not, create it\n    if not os.path.exists(DATA_DIR):\n        os.makedirs(DATA_DIR)\n    \n    # Create a file path for the stock data\n    file_path = os.path.join(DATA_DIR, f'{ticker}_{start_date}_{end_date}.csv')\n    \n    # Load or download data\n    if os.path.exists(file_path):\n        print(f\"Loading data from {file_path}\")\n        data = pd.read_csv(file_path, index_col='Date', parse_dates=True)\n    else:\n        print(f\"Downloading data for {ticker} from Yahoo finance\")\n        data = yf.download(ticker, start=start_date, end=end_date)\n        data.to_csv(file_path)\n        print(f\"Data saved to {file_path}\")\n    \n    result = {'data': data.copy()}\n    \n    # Handle NaN values\n    if dealNaN:\n        # Check for NaN values before handling\n        print(\"NaN values before handling\")\n        print(data.isna().sum())\n        # Depends on chosen drop or fill NaN\n        if drop_NaN is True:\n            data.dropna(inplace=True)\n        else:\n            data.fillna(data.mean(), inplace=True)\n        # Check for NaN values after handling\n        print(\"NaN values after handling\")\n        print(data.isna().sum())\n    \n    # Split data with specified method\n    if split_by_ratio:\n        print('Splitting by ratio')\n        train_samples = int((1-test_size) * len(data))\n        \n        result['data_train'] = data[:train_samples]\n        result['data_test'] = data[train_samples:]\n        \n        print(f\"Training samples: {len(result['data_train'])}\")\n        print(f\"Training samples: {len(result['data_test'])}\")\n        \n    elif split_by_date and split_date is not None:\n        print('Splitting by date')\n        split_date = pd.to_datetime(split_date)\n        \n        result['data_train'] = data[data.index <= split_date]\n        result['data_test'] = data[data.index > split_date]\n        \n        print(f\"Training samples: {len(result['data_train'])}\")\n        print(f\"Training samples: {len(result['data_test'])}\")\n    elif split_by_randomly:\n        print('Randomly splitting with shuffling')\n        result['data_train'], result['data_test'] = train_test_split(data, test_size=test_size, shuffle=True)\n        \n        print(f\"Training samples: {len(result['data_train'])}\")\n        print(f\"Training samples: {len(result['data_test'])}\")\n    \n    # Optionally scale feature columns\n    if scale:\n        scaler = MinMaxScaler(feature_range=(0,1))\n        data[feature_columns] = scaler.fit_transform(data[feature_columns])\n        result['scaled_data'] = data[feature_columns]\n        \n    return result"},"cell_type":"code","id":"a5254df9-0605-495d-b8f4-795c1bf06d79","outputs":[],"execution_count":42},{"source":"TICKER = 'AAPL'\nSTART_DATE = '2020-01-01'\nEND_DATE = '2023-01-01'\n\nresult = load_process_data(ticker=TICKER, start_date=START_DATE, end_date=END_DATE, split_by_date=False, split_by_randomly=False, split_by_ratio=True, split_date='06-01-2021', scale=True)\n\nprint(\"Train data in first 5 rows\")\nprint(result['data_train'].head())\nprint(\"Test data in first 5 rows\")\nprint(result['data_test'].head())","metadata":{"executionCancelledAt":null,"executionTime":102,"lastExecutedAt":1731474852558,"lastExecutedByKernel":"b758ff07-d9e6-42ac-a379-9fac2235c78a","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"TICKER = 'AAPL'\nSTART_DATE = '2020-01-01'\nEND_DATE = '2023-01-01'\n\nresult = load_process_data(ticker=TICKER, start_date=START_DATE, end_date=END_DATE, split_by_date=False, split_by_randomly=False, split_by_ratio=True, split_date='06-01-2021', scale=True)\n\nprint(\"Train data in first 5 rows\")\nprint(result['data_train'].head())\nprint(\"Test data in first 5 rows\")\nprint(result['data_test'].head())","outputsMetadata":{"0":{"height":38,"type":"stream"},"1":{"height":616,"type":"stream"},"2":{"height":38,"type":"stream"}}},"cell_type":"code","id":"f3ac8c78-559f-40c7-889e-59b49ba1d2e4","outputs":[{"output_type":"stream","name":"stderr","text":"[*********************100%%**********************]  1 of 1 completed"},{"output_type":"stream","name":"stdout","text":"Downloading data for AAPL from Yahoo finance\nData saved to stock_data/AAPL_2020-01-01_2023-01-01.csv\nNaN values before handling\nOpen         0\nHigh         0\nLow          0\nClose        0\nAdj Close    0\nVolume       0\ndtype: int64\nNaN values after handling\nOpen         0\nHigh         0\nLow          0\nClose        0\nAdj Close    0\nVolume       0\ndtype: int64\nSplitting by ratio\nTraining samples: 604\nTraining samples: 152\nTrain data in first 5 rows\n                 Open       High        Low      Close  Adj Close     Volume\nDate                                                                        \n2020-01-02  74.059998  75.150002  73.797501  75.087502  72.796028  135480400\n2020-01-03  74.287498  75.144997  74.125000  74.357498  72.088287  146322800\n2020-01-06  73.447502  74.989998  73.187500  74.949997  72.662704  118387200\n2020-01-07  74.959999  75.224998  74.370003  74.597504  72.320969  108872000\n2020-01-08  74.290001  76.110001  74.290001  75.797501  73.484344  132079200\nTest data in first 5 rows\n                  Open        High  ...   Adj Close     Volume\nDate                                ...                       \n2022-05-25  138.429993  141.789993  ...  138.634857   92482700\n2022-05-26  137.389999  144.339996  ...  141.851059   90601500\n2022-05-27  145.389999  149.679993  ...  147.632462   90978500\n2022-05-31  149.070007  150.660004  ...  146.843170  103718400\n2022-06-01  149.899994  151.740005  ...  146.714920   74286600\n\n[5 rows x 6 columns]\n"},{"output_type":"stream","name":"stderr","text":"\n"}],"execution_count":43},{"source":"result = load_process_data(ticker=TICKER, start_date=START_DATE, end_date=END_DATE, split_by_date=True, split_by_randomly=False, split_by_ratio=False, split_date='06-01-2021', scale=True)","metadata":{"executionCancelledAt":null,"executionTime":53,"lastExecutedAt":1731474852612,"lastExecutedByKernel":"b758ff07-d9e6-42ac-a379-9fac2235c78a","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"result = load_process_data(ticker=TICKER, start_date=START_DATE, end_date=END_DATE, split_by_date=True, split_by_randomly=False, split_by_ratio=False, split_date='06-01-2021', scale=True)","outputsMetadata":{"0":{"height":437,"type":"stream"}}},"cell_type":"code","id":"ad51a097-8f23-4abd-b9e5-74c6094fd8f3","outputs":[{"output_type":"stream","name":"stdout","text":"Loading data from stock_data/AAPL_2020-01-01_2023-01-01.csv\nNaN values before handling\nOpen         0\nHigh         0\nLow          0\nClose        0\nAdj Close    0\nVolume       0\ndtype: int64\nNaN values after handling\nOpen         0\nHigh         0\nLow          0\nClose        0\nAdj Close    0\nVolume       0\ndtype: int64\nSplitting by date\nTraining samples: 356\nTraining samples: 400\n"}],"execution_count":44},{"source":"result = load_process_data(ticker=TICKER, start_date=START_DATE, end_date=END_DATE, split_by_date=False, split_by_randomly=True, split_by_ratio=False, split_date='06-01-2021', scale=True)\n\nprint(\"Train data in first 5 rows\")\nprint(result['data_train'].head())\nprint(\"Test data in first 5 rows\")\nprint(result['data_test'].head())","metadata":{"executionCancelledAt":null,"executionTime":52,"lastExecutedAt":1731474852664,"lastExecutedByKernel":"b758ff07-d9e6-42ac-a379-9fac2235c78a","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"result = load_process_data(ticker=TICKER, start_date=START_DATE, end_date=END_DATE, split_by_date=False, split_by_randomly=True, split_by_ratio=False, split_date='06-01-2021', scale=True)\n\nprint(\"Train data in first 5 rows\")\nprint(result['data_train'].head())\nprint(\"Test data in first 5 rows\")\nprint(result['data_test'].head())","outputsMetadata":{"0":{"height":616,"type":"stream"}}},"cell_type":"code","id":"b9df7b8b-480a-44c8-976d-0be9b6c7daf0","outputs":[{"output_type":"stream","name":"stdout","text":"Loading data from stock_data/AAPL_2020-01-01_2023-01-01.csv\nNaN values before handling\nOpen         0\nHigh         0\nLow          0\nClose        0\nAdj Close    0\nVolume       0\ndtype: int64\nNaN values after handling\nOpen         0\nHigh         0\nLow          0\nClose        0\nAdj Close    0\nVolume       0\ndtype: int64\nRandomly splitting with shuffling\nTraining samples: 604\nTraining samples: 152\nTrain data in first 5 rows\n                  Open        High  ...   Adj Close     Volume\nDate                                ...                       \n2022-04-07  171.160004  173.360001  ...  169.581436   77594700\n2022-01-28  165.710007  170.350006  ...  167.584839  179935700\n2021-03-05  120.980003  121.940002  ...  118.908562  153766600\n2021-07-19  143.750000  144.070007  ...  139.740540  121434600\n2022-10-07  142.539993  143.100006  ...  138.402542   85925600\n\n[5 rows x 6 columns]\nTest data in first 5 rows\n                  Open        High  ...   Adj Close     Volume\nDate                                ...                       \n2020-02-14   81.184998   81.495003  ...   78.945251   80113600\n2020-08-06  110.404999  114.412498  ...  110.988197  202428800\n2022-05-04  159.669998  166.479996  ...  163.552444  108256500\n2020-11-13  119.440002  119.669998  ...  116.618988   81581900\n2022-04-18  163.919998  166.600006  ...  162.616547   69023900\n\n[5 rows x 6 columns]\n"}],"execution_count":45}],"metadata":{"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"editor":"DataLab"},"nbformat":4,"nbformat_minor":5}