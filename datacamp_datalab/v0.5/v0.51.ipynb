{"cells":[{"source":"# Stock Price Prediction with Flexible Model Saving and Loading\n\n# Import required libraries\nimport os\nimport pandas as pd\nimport yfinance as yf\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.layers import LSTM, GRU, Input, Dense, Dropout\n\n# %% Model Saving and Loading Utilities\ndef save_model_with_options(model, model_name, save_dir='saved_models'):\n    \"\"\"\n    Save a Keras model with flexible saving options.\n    \n    Parameters:\n    - model: Trained Keras model to save\n    - model_name: Name of the model file (without .h5 extension)\n    - save_dir: Directory to save the model (default is 'saved_models')\n    \n    Returns:\n    - Full path of the saved model\n    \"\"\"\n    # Create save directory if it doesn't exist\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n    \n    # Construct full save path\n    save_path = os.path.join(save_dir, f'{model_name}.h5')\n    \n    # Save the model\n    model.save(save_path)\n    print(f\"Model saved to {save_path}\")\n    \n    return save_path\n\ndef load_or_create_model(model_builder_func, model_name, \n                          load_existing=False, \n                          create_new=False, \n                          save_dir='saved_models', \n                          *args, **kwargs):\n    \"\"\"\n    Flexible function to either load an existing model or create a new one.\n    \"\"\"\n    save_path = os.path.join(save_dir, f'{model_name}.h5')\n    \n    # Try to load existing model if requested\n    if load_existing and os.path.exists(save_path):\n        try:\n            print(f\"Loading existing model from {save_path}\")\n            return load_model(save_path)\n        except Exception as e:\n            print(f\"Error loading model: {e}\")\n            print(\"Falling back to creating a new model.\")\n    \n    # Create a new model if requested\n    if create_new:\n        print(f\"Creating a new {model_name} model\")\n        model = model_builder_func(*args, **kwargs)\n        return model\n    \n    raise ValueError(\"No model loaded or created. Check your parameters.\")\n\n# %% Data Loading and Processing Function\ndef load_process_data(ticker, start_date, end_date, data_dir='stock_data',\n                     dealNaN=True, drop_NaN=True, split_by_ratio=False,\n                     split_by_date=False, split_by_randomly=True,\n                     split_date=None, scale=True, test_size=0.2):\n    \"\"\"Load and process stock data with various options for handling NaN and splitting\"\"\"\n    feature_columns = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\"]\n\n    # Create data directory if it doesn't exist\n    if not os.path.exists(data_dir):\n        os.makedirs(data_dir)\n\n    # Create file path for stock price data\n    file_path = os.path.join(data_dir, f'{ticker}_{start_date}_{end_date}.csv')\n\n    # Load or Download the data\n    if os.path.exists(file_path):\n        print(f\"Loading the data from {file_path}\")\n        data = pd.read_csv(file_path, index_col='Date', parse_dates=True)\n    else:\n        print(f\"Loading data {ticker} from Yahoo finance\")\n        data = yf.download(ticker, start_date, end_date)\n        data.to_csv(file_path)\n        print(f\"Data saved to {file_path}\")\n\n    result = {'data': data.copy()}\n    print(f\"Data Samples {len(data)}\")\n\n    # Handle NaN values\n    if dealNaN:\n        print(\"NaN values before handling\")\n        print(data.isna().sum())\n        if drop_NaN:\n            data.dropna(inplace=True)\n        else:\n            data.fillna(data.mean(), inplace=True)\n        print(\"NaN values after handling\")\n        print(data.isna().sum())\n\n    # Split data\n    if split_by_ratio:\n        print('Splitting by ratio')\n        train_samples = int((1-test_size) * len(data))\n        result['data_train'] = data[:train_samples]\n        result['data_test'] = data[train_samples:]\n    elif split_by_date:\n        print('Splitting by date')\n        split_date = pd.to_datetime(split_date)\n        result['data_train'] = data[data.index <= split_date]\n        result['data_test'] = data[data.index > split_date]\n    elif split_by_randomly:\n        print('Splitting randomly')\n        result['data_train'], result['data_test'] = train_test_split(\n            data, test_size=test_size, shuffle=True)\n\n    print(f\"Training samples: {len(result['data_train'])}\")\n    print(f\"Test samples: {len(result['data_test'])}\")\n\n    # Scale data if requested\n    if scale:\n        scalers = {}\n        scaled_data = pd.DataFrame(index=data.index)\n\n        for column in feature_columns:\n            scaler = MinMaxScaler(feature_range=(0, 1))\n            scaled_data[column] = scaler.fit_transform(\n                data[column].values.reshape(-1, 1)).flatten()\n            scalers[column] = scaler\n\n        result['scaled_data'] = scaled_data\n\n    return result, scalers\n\n# %% Data Preparation Functions\ndef multistep_data(scaler, n_steps, k):\n    X, y = [], []\n    scaler_array = scaler.values\n    for i in range(len(scaler_array) - n_steps - k + 1):\n        X.append(scaler_array[i:i + n_steps, :])\n        y.append(scaler_array[i + n_steps:i + n_steps + k, scaler.columns.get_loc('Close')])\n    \n    return np.array(X), np.array(y)\n\ndef multivariate_data(scaler, n_steps, feature_columns, target_column):\n    X, y = [], []\n    \n    scaler_array = scaler.values\n    for i in range(len(scaler_array) - n_steps):\n        X.append(scaler_array[i:i+n_steps])\n        y.append(scaler_array[i+n_steps, feature_columns.index(target_column)])\n    \n    return np.array(X), np.array(y)\n\ndef multivariate_multistep_data(scaler, features, target_column, n_steps, k):\n    X, y = [], []\n    \n    scaler_array = scaler.values\n    print(f\"Type {type(len(scaler_array))} and {type(n_steps)}: {n_steps}\")\n    for i in range(len(scaler_array) - n_steps - k + 1):\n        X.append(scaler_array[i:i+n_steps])\n        y.append(scaler_array[i+n_steps:i+n_steps + k, 0])\n    \n    return np.array(X), np.array(y)\n\n# %% Model Building Functions\ndef build_multistep_model(n_steps, k):\n    \"\"\"Build a multistep GRU model\"\"\"\n    model = Sequential()\n    model.add(Input(shape=(n_steps, 6)))\n    model.add(GRU(50, activation='relu'))\n    model.add(Dense(k))\n    \n    # Recreate the optimizer instance\n    optimizer = tf.keras.optimizers.Adam()\n\n    # Compile the model with the new optimizer\n    model.compile(optimizer=optimizer, loss='mean_squared_error')\n    return model\ndef build_multivariate_model(n_steps, n_features):\n    \"\"\"Build a multivariate GRU model\"\"\"\n    model = Sequential()\n    model.add(Input(shape=(n_steps, n_features)))\n    model.add(GRU(50, activation='relu'))\n    model.add(Dense(1))\n    \n    # Recreate the optimizer instance\n    optimizer = tf.keras.optimizers.Adam()\n\n    # Compile the model with the new optimizer\n    model.compile(optimizer=optimizer, loss='mean_squared_error')\n    return model\n    # return model\n\ndef build_multivariate_multistep_model(n_steps, n_features, k):\n    \"\"\"Build a multivariate multistep GRU model\"\"\"\n    model = Sequential()\n    model.add(Input(shape=(n_steps, n_features)))\n    model.add(LSTM(50, activation='relu'))\n    model.add(Dense(k))\n    # Recreate the optimizer instance\n    optimizer = tf.keras.optimizers.Adam()\n\n    # Compile the model with the new optimizer\n    model.compile(optimizer=optimizer, loss='mean_squared_error')\n    return model\n\n# %% Configuration Parameters\nTICKER = 'AAPL'\nSTART_DATE = '2020-01-01'\nEND_DATE = '2023-01-01'\nN_STEPS = 30\nK = 5\nFEATURE_COLUMNS = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\"]\nTARGET_COLUMN = 'Close'\n\n# %% Load and Process Data\nresult, scalers = load_process_data(\n    ticker=TICKER,\n    start_date=START_DATE,\n    end_date=END_DATE,\n    split_by_date=False,\n    split_by_randomly=False,\n    split_by_ratio=True,\n    split_date='2021-06-01',\n    scale=True\n)\n\n# %% Multistep Model Workflow\ndef run_multistep_model_workflow(load_existing=True, create_new=False):\n    # Prepare Data\n    X, y = multistep_data(result['scaled_data'], N_STEPS, K)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n    \n    print(f\"shape {X_train.shape} and {y_train.shape}\")\n    \n    # Model Handling with Flexible Options\n    multistep_model = load_or_create_model(\n        model_builder_func=build_multistep_model, \n        model_name='GRU_multistep', \n        load_existing=load_existing,  # Set to True to load existing model\n        create_new=create_new,\n        n_steps=N_STEPS, \n        k=K\n    )\n    \n    # Recreate the optimizer instance\n    optimizer = tf.keras.optimizers.Adam()\n    \n    # Compile the model with the new optimizer\n    multistep_model.compile(optimizer=optimizer, loss='mean_squared_error')\n    \n    # Train Model\n    if create_new or not load_existing:\n        multistep_model.fit(\n            X_train, y_train, \n            epochs=50, \n            batch_size=32, \n            validation_data=(X_test, y_test), \n            verbose=1\n        )\n    \n        # Save Model\n        save_model_with_options(multistep_model, 'GRU_multistep')\n    \n    # Prediction and Visualization\n    y_pred_scaled = multistep_model.predict(X_test)\n    y_pred = scalers['Close'].inverse_transform(y_pred_scaled)\n    y_test_original = scalers['Close'].inverse_transform(y_test)\n    \n    # Plotting\n    plt.figure(figsize=(10, 6))\n    plt.plot(y_pred.flatten(), label='Predicted', linestyle='--')\n    plt.plot(y_test_original.flatten(), label='Actual', linestyle='-')\n    plt.title('Multistep: Predicted vs Actual Closing Prices')\n    plt.xlabel('Number of Observations')\n    plt.ylabel('Closing Price')\n    plt.legend()\n    plt.show()\n    \n    return multistep_model\n\ndef run_multivariate_model_workflow(load_existing=True, create_new=False):\n    # Prepare Data\n    X, y = multivariate_data(result['scaled_data'], N_STEPS, FEATURE_COLUMNS, TARGET_COLUMN)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n    \n    print(f\"shape {X_train.shape} and {X_test.shape} and {y_train.shape} and {y_test.shape}\")\n    \n    # Model Handling with Flexible Options\n    multivariate_model = load_or_create_model(\n        model_builder_func=build_multivariate_model, \n        model_name='GRU_multivariate', \n        load_existing=load_existing,  # Set to True to load existing model\n        create_new=create_new,\n        n_steps=N_STEPS, \n        n_features=len(FEATURE_COLUMNS)\n    )\n    \n    # Recreate the optimizer instance\n    optimizer = tf.keras.optimizers.Adam()\n    \n    # Compile the model with the new optimizer\n    multivariate_model.compile(optimizer=optimizer, loss='mean_squared_error')\n    \n    # Train Model\n    if create_new or not load_existing:\n        multivariate_model.fit(\n            X_train, y_train, \n            epochs=50, \n            batch_size=32, \n            validation_data=(X_test, y_test), \n            verbose=1\n        )\n    \n        # Save Model\n        save_model_with_options(multivariate_model, 'GRU_multivariate')\n    \n    # Prediction and Visualization\n    y_pred_scaled = multivariate_model.predict(X_test)\n    target_column_index = FEATURE_COLUMNS.index('Close')\n    \n    y_pred_scaled_expanded = np.zeros((y_pred_scaled.shape[0], X_test.shape[2]))\n    y_pred_scaled_expanded[:, target_column_index] = y_pred_scaled[:, 0]\n    \n    y_test_expanded = np.zeros((y_test.shape[0], X_test.shape[2]))\n    y_test_expanded[:, target_column_index] = y_test\n    \n    # Inverse transform for 'Close' column\n    y_pred_original = scalers['Close'].inverse_transform(\n        y_pred_scaled_expanded[:, target_column_index].reshape(-1, 1)\n    ).flatten()\n    \n    y_test_original = scalers['Close'].inverse_transform(\n        y_test_expanded[:, target_column_index].reshape(-1, 1)\n    ).flatten()\n    \n    # Plotting\n    plt.figure(figsize=(10, 6))\n    plt.plot(y_pred_original, label='Predicted', linestyle='--')\n    plt.plot(y_test_original, label='Actual', linestyle='-')\n    plt.title('Multivariate: Predicted vs Actual Closing Prices')\n    plt.xlabel('Number of Observations')\n    plt.ylabel('Closing Price')\n    plt.legend()\n    plt.show()\n    \n    return multivariate_model\n\ndef run_multivariate_multistep_model_workflow(load_existing=True, create_new=False):\n    # Prepare Data\n    X, y = multivariate_multistep_data(result['scaled_data'], N_STEPS, FEATURE_COLUMNS, N_STEPS, K)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n    \n    # Model Handling with Flexible Options\n    multivariate_multistep_model = load_or_create_model(\n        model_builder_func=build_multivariate_multistep_model, \n        model_name='GRU_multivariate_multistep', \n        load_existing=load_existing,  # Set to True to load existing model\n        create_new=create_new,\n        n_steps=N_STEPS, \n        n_features=len(FEATURE_COLUMNS),\n        k=K\n    )\n    \n    # Recreate the optimizer instance\n    optimizer = tf.keras.optimizers.Adam()\n    \n    # Compile the model with the new optimizer\n    multivariate_multistep_model.compile(optimizer=optimizer, loss='mean_squared_error')\n    \n    # Train Model\n    if create_new or not load_existing:\n        multivariate_multistep_model.fit(\n            X_train, y_train, \n            epochs=50, \n            batch_size=32, \n            validation_data=(X_test, y_test), \n            verbose=1\n        )\n    \n        # Save Model\n        save_model_with_options(multivariate_multistep_model, 'GRU_multivariate_multistep')\n    \n    # Prediction and Visualization\n    y_pred_scaled = multivariate_multistep_model.predict(X_test)\n    y_pred = scalers['Close'].inverse_transform(y_pred_scaled)\n    y_test_original = scalers['Close'].inverse_transform(y_test)\n    \n    y_pred_df3 = pd.DataFrame({\n    'Predicted' : y_pred.flatten(),\n    'Actual' : y_test_original.flatten()\n})\n    y_pred_df3\n    \n    # Plotting\n    plt.figure(figsize=(10, 6))\n    plt.plot(y_pred_df3['Predicted'], label='Predicted', linestyle='--')\n    plt.plot(y_pred_df3['Actual'], label='Actual', linestyle='-')\n    plt.title('Multivariate and Multistep: Predicted vs Actual Closing Prices')\n    plt.xlabel('Number of Observations')\n    plt.ylabel('Closing Price')\n    plt.legend()\n    plt.show()\n    \n    return multivariate_multistep_model\n\n# %% Run Multistep Model Workflow\nmultistep_model = run_multistep_model_workflow(load_existing=True, create_new=False)\n\n# %% Run Multivariate Model Workflow\nmultivariate_model = run_multivariate_model_workflow(load_existing=True, create_new=True)\n\nmultivariate_multistep_model = run_multivariate_multistep_model_workflow(load_existing=True, create_new=False)\n\nprint(\"Model workflows completed successfully!\")","metadata":{"executionCancelledAt":null,"executionTime":5888,"lastExecutedAt":1732760067044,"lastExecutedByKernel":"b1e48ddf-a4be-4ab1-984b-f7a74ddfeb11","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Stock Price Prediction with Flexible Model Saving and Loading\n\n# Import required libraries\nimport os\nimport pandas as pd\nimport yfinance as yf\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.layers import LSTM, GRU, Input, Dense, Dropout\n\n# %% Model Saving and Loading Utilities\ndef save_model_with_options(model, model_name, save_dir='saved_models'):\n    \"\"\"\n    Save a Keras model with flexible saving options.\n    \n    Parameters:\n    - model: Trained Keras model to save\n    - model_name: Name of the model file (without .h5 extension)\n    - save_dir: Directory to save the model (default is 'saved_models')\n    \n    Returns:\n    - Full path of the saved model\n    \"\"\"\n    # Create save directory if it doesn't exist\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n    \n    # Construct full save path\n    save_path = os.path.join(save_dir, f'{model_name}.h5')\n    \n    # Save the model\n    model.save(save_path)\n    print(f\"Model saved to {save_path}\")\n    \n    return save_path\n\ndef load_or_create_model(model_builder_func, model_name, \n                          load_existing=False, \n                          create_new=False, \n                          save_dir='saved_models', \n                          *args, **kwargs):\n    \"\"\"\n    Flexible function to either load an existing model or create a new one.\n    \"\"\"\n    save_path = os.path.join(save_dir, f'{model_name}.h5')\n    \n    # Try to load existing model if requested\n    if load_existing and os.path.exists(save_path):\n        try:\n            print(f\"Loading existing model from {save_path}\")\n            return load_model(save_path)\n        except Exception as e:\n            print(f\"Error loading model: {e}\")\n            print(\"Falling back to creating a new model.\")\n    \n    # Create a new model if requested\n    if create_new:\n        print(f\"Creating a new {model_name} model\")\n        model = model_builder_func(*args, **kwargs)\n        return model\n    \n    raise ValueError(\"No model loaded or created. Check your parameters.\")\n\n# %% Data Loading and Processing Function\ndef load_process_data(ticker, start_date, end_date, data_dir='stock_data',\n                     dealNaN=True, drop_NaN=True, split_by_ratio=False,\n                     split_by_date=False, split_by_randomly=True,\n                     split_date=None, scale=True, test_size=0.2):\n    \"\"\"Load and process stock data with various options for handling NaN and splitting\"\"\"\n    feature_columns = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\"]\n\n    # Create data directory if it doesn't exist\n    if not os.path.exists(data_dir):\n        os.makedirs(data_dir)\n\n    # Create file path for stock price data\n    file_path = os.path.join(data_dir, f'{ticker}_{start_date}_{end_date}.csv')\n\n    # Load or Download the data\n    if os.path.exists(file_path):\n        print(f\"Loading the data from {file_path}\")\n        data = pd.read_csv(file_path, index_col='Date', parse_dates=True)\n    else:\n        print(f\"Loading data {ticker} from Yahoo finance\")\n        data = yf.download(ticker, start_date, end_date)\n        data.to_csv(file_path)\n        print(f\"Data saved to {file_path}\")\n\n    result = {'data': data.copy()}\n    print(f\"Data Samples {len(data)}\")\n\n    # Handle NaN values\n    if dealNaN:\n        print(\"NaN values before handling\")\n        print(data.isna().sum())\n        if drop_NaN:\n            data.dropna(inplace=True)\n        else:\n            data.fillna(data.mean(), inplace=True)\n        print(\"NaN values after handling\")\n        print(data.isna().sum())\n\n    # Split data\n    if split_by_ratio:\n        print('Splitting by ratio')\n        train_samples = int((1-test_size) * len(data))\n        result['data_train'] = data[:train_samples]\n        result['data_test'] = data[train_samples:]\n    elif split_by_date:\n        print('Splitting by date')\n        split_date = pd.to_datetime(split_date)\n        result['data_train'] = data[data.index <= split_date]\n        result['data_test'] = data[data.index > split_date]\n    elif split_by_randomly:\n        print('Splitting randomly')\n        result['data_train'], result['data_test'] = train_test_split(\n            data, test_size=test_size, shuffle=True)\n\n    print(f\"Training samples: {len(result['data_train'])}\")\n    print(f\"Test samples: {len(result['data_test'])}\")\n\n    # Scale data if requested\n    if scale:\n        scalers = {}\n        scaled_data = pd.DataFrame(index=data.index)\n\n        for column in feature_columns:\n            scaler = MinMaxScaler(feature_range=(0, 1))\n            scaled_data[column] = scaler.fit_transform(\n                data[column].values.reshape(-1, 1)).flatten()\n            scalers[column] = scaler\n\n        result['scaled_data'] = scaled_data\n\n    return result, scalers\n\n# %% Data Preparation Functions\ndef multistep_data(scaler, n_steps, k):\n    X, y = [], []\n    scaler_array = scaler.values\n    for i in range(len(scaler_array) - n_steps - k + 1):\n        X.append(scaler_array[i:i + n_steps, :])\n        y.append(scaler_array[i + n_steps:i + n_steps + k, scaler.columns.get_loc('Close')])\n    \n    return np.array(X), np.array(y)\n\ndef multivariate_data(scaler, n_steps, feature_columns, target_column):\n    X, y = [], []\n    \n    scaler_array = scaler.values\n    for i in range(len(scaler_array) - n_steps):\n        X.append(scaler_array[i:i+n_steps])\n        y.append(scaler_array[i+n_steps, feature_columns.index(target_column)])\n    \n    return np.array(X), np.array(y)\n\ndef multivariate_multistep_data(scaler, features, target_column, n_steps, k):\n    X, y = [], []\n    \n    scaler_array = scaler.values\n    print(f\"Type {type(len(scaler_array))} and {type(n_steps)}: {n_steps}\")\n    for i in range(len(scaler_array) - n_steps - k + 1):\n        X.append(scaler_array[i:i+n_steps])\n        y.append(scaler_array[i+n_steps:i+n_steps + k, 0])\n    \n    return np.array(X), np.array(y)\n\n# %% Model Building Functions\ndef build_multistep_model(n_steps, k):\n    \"\"\"Build a multistep GRU model\"\"\"\n    model = Sequential()\n    model.add(Input(shape=(n_steps, 6)))\n    model.add(GRU(50, activation='relu'))\n    model.add(Dense(k))\n    \n    # Recreate the optimizer instance\n    optimizer = tf.keras.optimizers.Adam()\n\n    # Compile the model with the new optimizer\n    model.compile(optimizer=optimizer, loss='mean_squared_error')\n    return model\ndef build_multivariate_model(n_steps, n_features):\n    \"\"\"Build a multivariate GRU model\"\"\"\n    model = Sequential()\n    model.add(Input(shape=(n_steps, n_features)))\n    model.add(GRU(50, activation='relu'))\n    model.add(Dense(1))\n    \n    # Recreate the optimizer instance\n    optimizer = tf.keras.optimizers.Adam()\n\n    # Compile the model with the new optimizer\n    model.compile(optimizer=optimizer, loss='mean_squared_error')\n    return model\n    # return model\n\ndef build_multivariate_multistep_model(n_steps, n_features, k):\n    \"\"\"Build a multivariate multistep GRU model\"\"\"\n    model = Sequential()\n    model.add(Input(shape=(n_steps, n_features)))\n    model.add(LSTM(50, activation='relu'))\n    model.add(Dense(k))\n    # Recreate the optimizer instance\n    optimizer = tf.keras.optimizers.Adam()\n\n    # Compile the model with the new optimizer\n    model.compile(optimizer=optimizer, loss='mean_squared_error')\n    return model\n\n# %% Configuration Parameters\nTICKER = 'AAPL'\nSTART_DATE = '2020-01-01'\nEND_DATE = '2023-01-01'\nN_STEPS = 30\nK = 5\nFEATURE_COLUMNS = [\"Open\", \"High\", \"Low\", \"Close\", \"Adj Close\", \"Volume\"]\nTARGET_COLUMN = 'Close'\n\n# %% Load and Process Data\nresult, scalers = load_process_data(\n    ticker=TICKER,\n    start_date=START_DATE,\n    end_date=END_DATE,\n    split_by_date=False,\n    split_by_randomly=False,\n    split_by_ratio=True,\n    split_date='2021-06-01',\n    scale=True\n)\n\n# %% Multistep Model Workflow\ndef run_multistep_model_workflow(load_existing=True, create_new=False):\n    # Prepare Data\n    X, y = multistep_data(result['scaled_data'], N_STEPS, K)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n    \n    print(f\"shape {X_train.shape} and {y_train.shape}\")\n    \n    # Model Handling with Flexible Options\n    multistep_model = load_or_create_model(\n        model_builder_func=build_multistep_model, \n        model_name='GRU_multistep', \n        load_existing=load_existing,  # Set to True to load existing model\n        create_new=create_new,\n        n_steps=N_STEPS, \n        k=K\n    )\n    \n    # Recreate the optimizer instance\n    optimizer = tf.keras.optimizers.Adam()\n    \n    # Compile the model with the new optimizer\n    multistep_model.compile(optimizer=optimizer, loss='mean_squared_error')\n    \n    # Train Model\n    if create_new or not load_existing:\n        multistep_model.fit(\n            X_train, y_train, \n            epochs=50, \n            batch_size=32, \n            validation_data=(X_test, y_test), \n            verbose=1\n        )\n    \n        # Save Model\n        save_model_with_options(multistep_model, 'GRU_multistep')\n    \n    # Prediction and Visualization\n    y_pred_scaled = multistep_model.predict(X_test)\n    y_pred = scalers['Close'].inverse_transform(y_pred_scaled)\n    y_test_original = scalers['Close'].inverse_transform(y_test)\n    \n    # Plotting\n    plt.figure(figsize=(10, 6))\n    plt.plot(y_pred.flatten(), label='Predicted', linestyle='--')\n    plt.plot(y_test_original.flatten(), label='Actual', linestyle='-')\n    plt.title('Multistep: Predicted vs Actual Closing Prices')\n    plt.xlabel('Number of Observations')\n    plt.ylabel('Closing Price')\n    plt.legend()\n    plt.show()\n    \n    return multistep_model\n\ndef run_multivariate_model_workflow(load_existing=True, create_new=False):\n    # Prepare Data\n    X, y = multivariate_data(result['scaled_data'], N_STEPS, FEATURE_COLUMNS, TARGET_COLUMN)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n    \n    print(f\"shape {X_train.shape} and {X_test.shape} and {y_train.shape} and {y_test.shape}\")\n    \n    # Model Handling with Flexible Options\n    multivariate_model = load_or_create_model(\n        model_builder_func=build_multivariate_model, \n        model_name='GRU_multivariate', \n        load_existing=load_existing,  # Set to True to load existing model\n        create_new=create_new,\n        n_steps=N_STEPS, \n        n_features=len(FEATURE_COLUMNS)\n    )\n    \n    # Recreate the optimizer instance\n    optimizer = tf.keras.optimizers.Adam()\n    \n    # Compile the model with the new optimizer\n    multivariate_model.compile(optimizer=optimizer, loss='mean_squared_error')\n    \n    # Train Model\n    if create_new or not load_existing:\n        multivariate_model.fit(\n            X_train, y_train, \n            epochs=50, \n            batch_size=32, \n            validation_data=(X_test, y_test), \n            verbose=1\n        )\n    \n        # Save Model\n        save_model_with_options(multivariate_model, 'GRU_multivariate')\n    \n    # Prediction and Visualization\n    y_pred_scaled = multivariate_model.predict(X_test)\n    target_column_index = FEATURE_COLUMNS.index('Close')\n    \n    y_pred_scaled_expanded = np.zeros((y_pred_scaled.shape[0], X_test.shape[2]))\n    y_pred_scaled_expanded[:, target_column_index] = y_pred_scaled[:, 0]\n    \n    y_test_expanded = np.zeros((y_test.shape[0], X_test.shape[2]))\n    y_test_expanded[:, target_column_index] = y_test\n    \n    # Inverse transform for 'Close' column\n    y_pred_original = scalers['Close'].inverse_transform(\n        y_pred_scaled_expanded[:, target_column_index].reshape(-1, 1)\n    ).flatten()\n    \n    y_test_original = scalers['Close'].inverse_transform(\n        y_test_expanded[:, target_column_index].reshape(-1, 1)\n    ).flatten()\n    \n    # Plotting\n    plt.figure(figsize=(10, 6))\n    plt.plot(y_pred_original, label='Predicted', linestyle='--')\n    plt.plot(y_test_original, label='Actual', linestyle='-')\n    plt.title('Multivariate: Predicted vs Actual Closing Prices')\n    plt.xlabel('Number of Observations')\n    plt.ylabel('Closing Price')\n    plt.legend()\n    plt.show()\n    \n    return multivariate_model\n\ndef run_multivariate_multistep_model_workflow(load_existing=True, create_new=False):\n    # Prepare Data\n    X, y = multivariate_multistep_data(result['scaled_data'], N_STEPS, FEATURE_COLUMNS, N_STEPS, K)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n    \n    # Model Handling with Flexible Options\n    multivariate_multistep_model = load_or_create_model(\n        model_builder_func=build_multivariate_multistep_model, \n        model_name='GRU_multivariate_multistep', \n        load_existing=load_existing,  # Set to True to load existing model\n        create_new=create_new,\n        n_steps=N_STEPS, \n        n_features=len(FEATURE_COLUMNS),\n        k=K\n    )\n    \n    # Recreate the optimizer instance\n    optimizer = tf.keras.optimizers.Adam()\n    \n    # Compile the model with the new optimizer\n    multivariate_multistep_model.compile(optimizer=optimizer, loss='mean_squared_error')\n    \n    # Train Model\n    if create_new or not load_existing:\n        multivariate_multistep_model.fit(\n            X_train, y_train, \n            epochs=50, \n            batch_size=32, \n            validation_data=(X_test, y_test), \n            verbose=1\n        )\n    \n        # Save Model\n        save_model_with_options(multivariate_multistep_model, 'GRU_multivariate_multistep')\n    \n    # Prediction and Visualization\n    y_pred_scaled = multivariate_multistep_model.predict(X_test)\n    y_pred = scalers['Close'].inverse_transform(y_pred_scaled)\n    y_test_original = scalers['Close'].inverse_transform(y_test)\n    \n    y_pred_df3 = pd.DataFrame({\n    'Predicted' : y_pred.flatten(),\n    'Actual' : y_test_original.flatten()\n})\n    y_pred_df3\n    \n    # Plotting\n    plt.figure(figsize=(10, 6))\n    plt.plot(y_pred_df3['Predicted'], label='Predicted', linestyle='--')\n    plt.plot(y_pred_df3['Actual'], label='Actual', linestyle='-')\n    plt.title('Multivariate and Multistep: Predicted vs Actual Closing Prices')\n    plt.xlabel('Number of Observations')\n    plt.ylabel('Closing Price')\n    plt.legend()\n    plt.show()\n    \n    return multivariate_multistep_model\n\n# %% Run Multistep Model Workflow\nmultistep_model = run_multistep_model_workflow(load_existing=True, create_new=False)\n\n# %% Run Multivariate Model Workflow\nmultivariate_model = run_multivariate_model_workflow(load_existing=True, create_new=True)\n\nmultivariate_multistep_model = run_multivariate_multistep_model_workflow(load_existing=True, create_new=False)\n\nprint(\"Model workflows completed successfully!\")","outputsMetadata":{"0":{"height":101,"type":"stream"},"1":{"height":479,"type":"stream"}}},"cell_type":"code","id":"47edceb2-29d7-41c1-a169-0e6b2add7787","outputs":[{"output_type":"stream","name":"stderr","text":"2024-11-28 02:14:24.641705: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"},{"output_type":"stream","name":"stdout","text":"Loading the data from stock_data/AAPL_2020-01-01_2023-01-01.csv\nData Samples 756\nNaN values before handling\nOpen         0\nHigh         0\nLow          0\nClose        0\nAdj Close    0\nVolume       0\ndtype: int64\nNaN values after handling\nOpen         0\nHigh         0\nLow          0\nClose        0\nAdj Close    0\nVolume       0\ndtype: int64\nSplitting by ratio\nTraining samples: 604\nTest samples: 152\nshape (577, 30, 6) and (577, 5)\n"},{"output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 415\u001b[0m\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m multivariate_multistep_model\n\u001b[1;32m    414\u001b[0m \u001b[38;5;66;03m# %% Run Multistep Model Workflow\u001b[39;00m\n\u001b[0;32m--> 415\u001b[0m multistep_model \u001b[38;5;241m=\u001b[39m \u001b[43mrun_multistep_model_workflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mload_existing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_new\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;66;03m# %% Run Multivariate Model Workflow\u001b[39;00m\n\u001b[1;32m    418\u001b[0m multivariate_model \u001b[38;5;241m=\u001b[39m run_multivariate_model_workflow(load_existing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, create_new\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n","Cell \u001b[0;32mIn[1], line 244\u001b[0m, in \u001b[0;36mrun_multistep_model_workflow\u001b[0;34m(load_existing, create_new)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_train\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_train\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    243\u001b[0m \u001b[38;5;66;03m# Model Handling with Flexible Options\u001b[39;00m\n\u001b[0;32m--> 244\u001b[0m multistep_model \u001b[38;5;241m=\u001b[39m \u001b[43mload_or_create_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_builder_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbuild_multistep_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGRU_multistep\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_existing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_existing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Set to True to load existing model\u001b[39;49;00m\n\u001b[1;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_new\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_new\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_STEPS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mK\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# Recreate the optimizer instance\u001b[39;00m\n\u001b[1;32m    254\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam()\n","Cell \u001b[0;32mIn[1], line 66\u001b[0m, in \u001b[0;36mload_or_create_model\u001b[0;34m(model_builder_func, model_name, load_existing, create_new, save_dir, *args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m     model \u001b[38;5;241m=\u001b[39m model_builder_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m---> 66\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo model loaded or created. Check your parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mValueError\u001b[0m: No model loaded or created. Check your parameters."],"ename":"ValueError","evalue":"No model loaded or created. Check your parameters."}],"execution_count":null}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"editor":"DataLab"},"nbformat":4,"nbformat_minor":5}